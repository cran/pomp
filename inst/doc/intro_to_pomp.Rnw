\documentclass[10pt,reqno,final]{amsart}
%\VignetteIndexEntry{Introduction to pomp}
\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[round]{natbib}
\usepackage{paralist}
\usepackage{float}

\setlength{\textwidth}{6.25in}
\setlength{\textheight}{8.75in}
\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{-0.35in}
\setlength{\parskip}{0.1in}  
\setlength{\parindent}{0.0in}  
\setcounter{secnumdepth}{1}
\setcounter{tocdepth}{1}

\floatstyle{ruled}
\newfloat{textbox}{t}{tbx}
\floatname{textbox}{Box}

\newcommand\code[1]{\texttt{#1}}
\newcommand{\R}{\textsf{R}}
\newcommand{\pomp}{\texttt{pomp}}
\newcommand{\expect}[1]{\mathbb{E}\left[#1\right]}

\title[Introduction to pomp]{Introduction to pomp:\\ inference for partially-observed Markov processes}

\author[King]{Aaron A. King}
\author[Ionides]{Edward L. Ionides}
\author[Bret\'o]{Carles Bret\'o}
\author[Ellner]{Stephen P. Ellner}
\author[Kendall]{Bruce E. Kendall}
\author[Ferrari]{Matthew Ferrari}
\author[Lavine]{Michael L. Lavine}
\author[Reuman]{Daniel C. Reuman}

\address{
  A. A. King,
  Departments of Ecology \& Evolutionary Biology and Mathematics, 
  University of Michigan, 
  Ann Arbor, Michigan
  48109-1048 USA
}

\email{kingaa at umich dot edu} 

\urladdr{http://pomp.r-forge.r-project.org}

\date{\today, \pomp~version~\Sexpr{packageDescription("pomp",fields="Version")}}

\SweaveOpts{echo=T,results=verbatim,print=F,eps=F,pdf=T,keep.source=T}

\begin{document}

\thispagestyle{empty}

\maketitle

\tableofcontents

<<set-opts,echo=F,results=hide>>=
  glop <- options(keep.source=TRUE,width=60,continue=" ",prompt=" ")
  library(pomp)
  pdf.options(useDingbats=FALSE)
  set.seed(5384959)
@ 

\section{Partially-observed Markov processes}

Partially-observed Markov process models are also known as state-space models or stochastic dynamical systems.
The \R\ package \pomp\ provides facilities for fitting such models to uni- or multi-variate time series, for simulating them, for assessing model adequacy, and for comparing among models.
The methods implemented in \pomp\ are all ``plug-and-play'' in the sense that they require only that one be able to simulate the process portion of the model.
This property is desirable because it will typically be the case that a mechanistic model will not be otherwise amenable to standard statistical analyses, but will be relatively easy to simulate.
Even when one is interested in a model for which one can write down an explicit likelihood, for example, there are probably models that are ``nearby'' and equally interesting for which the likelihood cannot explicitly be written.
The price one pays for this flexibility is primarily in terms of computational expense.
%% more on: why plug-and-play: flexibility of model choice aids scientific inference by allowing us to entertain multiple competing hypotheses
%% ability to fit a variety of alternative models using the same statistical/computational approach makes direct comparison easier

A partially-observed Markov process has two parts.
First, there is the true underlying process which is generating the data.
This is typically the thing we are most interested in: our goal is usually to better understand this process.
Specifically, we may have various alternate hypotheses about how this system functions and we want to see whether time series data can tell us which hypotheses explain the data better.
The challenge, of course, is that the data shed light on the system only indirectly.

\pomp\ assumes that we can translate our hypotheses about the underlying, unobserved process into a Markov process model:
That is, we are willing to assume that the system has a true \emph{state} process, $X_t$ that is Markovian.
In particular, given any sequence of times $t_0$, $t_1$, \dots, $t_n$, the Markov property allows us to write
\begin{equation}\label{eq:state-process}
  X_{t_{k+1}}\;\sim\;f(X_{t_{k}},\theta),
\end{equation}
for each $k=1,\dots,n$, where $f$ is some density.
[In this document, we will be fairly cavalier about abusing notation, using the letter $f$ to denote a probability distribution function generically, assuming that the reader will be able to unambiguously tell which probability distribution we're talking about from the arguments to $f$ and the context.]
That is, we assume that the state at time $t_{k+1}$ depends only on the state at time $t_{k}$ and on some parameters $\theta$.

In addition to the state process $X_t$, there is some measurement or observation process $Y_t$ which models the process by which the data themselves are generated and links the data therefore to the state process.
In particular, we assume that
\begin{equation}\label{eq:meas-process}
  Y_t\;\sim\;f(X_t,\theta)
\end{equation}
for all times $t$.
That is, that the observations $Y_t$ are random variables that depend only on the state \emph{at that time} as well as on some parameters.

So, to specify a partially-observed Markov process model, one has to specify a process (unobserved or state) model and a measurement (observation) model.
This seems straightforward enough, but from the computational point of view, there are actually two aspects to each model that may be important.
On the one hand, one may need to \emph{evaluate} the probability density of the state-transition $X_{t_{k}}\;\to\;X_{t_{k+1}}$, i.e., to compute $f(X_{t_{k+1}}\,\vert\,X_{t_{k}},\theta)$.
On the other hand, one may need to \emph{simulate} this distribution, i.e., to draw random samples from the distribution of $X_{t_{k+1}}\;\vert\;X_{t_k}$.
Depending on the model and on what one wants specifically to do, it may be technically easier or harder to do one of these or the other.
Likewise, one may want to simulate, or evaluate the likelihood of, observations $Y_t$.
At its most basic level \pomp\ is an infrastructure that allows you to encode your model by specifying some or all of these four basic components:
\begin{compactdesc}
\item[\code{rprocess}] a simulator of the process model,
\item[\code{dprocess}] an evaluator of the process model probability density function,
\item[\code{rmeasure}] a simulator of the measurement model, and
\item[\code{dmeasure}] an evaluator of the measurement model probability density function.
\end{compactdesc}
Once you've encoded your model, \pomp\ provides a number of algorithms you can use to work with it.
In particular, within \pomp, you can:
\begin{compactenum}[(1)]
\item simulate your model easily, using \code{simulate},
\item integrate your model's deterministic skeleton, using \code{trajectory},
\item estimate the likelihood for any given set of parameters using sequential Monte Carlo, implemented in \code{pfilter},
\item find maximum likelihood estimates for parameters using iterated filtering, implemented in \code{mif},
\item estimate parameters using a simulated quasi-maximum-likelihood approach called \emph{nonlinear forecasting}, implemented in \code{nlf},
\item estimate parameters using trajectory matching, as implemented in \code{traj.match},
\item estimate parameters using probe matching, as implemented in \code{probe.match},
\item print and plot data, simulations, and diagnostics for the foregoing algorithms,
\item build new algorithms for partially observed Markov processes upon the foundations \pomp\ provides, using the package's applications programming interface (API).
\end{compactenum}
In this document, we'll see how all this works using relatively simple examples.

\section{A first example: a simple discrete-time population model.}

We'll demonstrate the basics of \pomp\ using a very simple discrete-time model.
The plug-and-play methods in \pomp\ were designed to work on more complicated models, and for our first example, they'll be extreme overkill, but starting with a simple model will help make the implementation of more general models clear.
Moreover, our first example will be one for which plug-and-play methods are not even necessary.
This will allow us to compare the results from generalizable plug-and-play methods with exact results from specialized methods appropriate to this particular model.
Later we'll look at a continuous-time model for which no such special tricks are available.

Consider the discrete-time Gompertz model of population growth.
Under this model, the density, $X_{t+1}$, of a population of plants or animals at time $t+{\Delta}t$ depends on the density, $X_{t}$, at time $t$ according to
\begin{equation}\label{eq:gompertz1}
  X_{t+1}=K^{1-e^{-r\,{\Delta}t}}\,X_{t}^{e^{-r\,{\Delta}t}}\,\varepsilon_{t},
\end{equation}
where $K$ is the so-called ``carrying capacity'' of the population, $r$ is a positive parameter, and the $\varepsilon_{t}$ are independent and identically-distributed lognormal random variables.
In different notation, this model is
\begin{equation}\label{eq:gompertz2}
  \log{X_{t+1}}\;\sim\;\mathrm{normal}(\log{K}+\log{\left(\frac{X_t}{K}\right)}\,e^{-r\,{\Delta}t},\sigma),
\end{equation}
where $\sigma^2={\mathrm{Var}[\log{\epsilon_{t}}]}$.
We'll assume that we can measure the population density only with error.
In particular, we'll assume that errors in measurement are lognormally distributed:
\begin{equation}\label{eq:gompertz-obs}
  \log{Y_{t}}\;\sim\;\mathrm{normal}(\log{X_{t}},\tau).
\end{equation}

As we noted above, for this particular model, it isn't necessary to use plug-and-play methods: 
one can obtain exact maximum likelihood estimates of this model's parameters using the Kalman filter.
We will demonstrate this below and use it to check the results of plug-and-play inference.
For now, let's approach this model as we would a more complex model for which no such exact estimation is available.

\section{Defining a partially observed Markov process in \pomp.}

In order to fully specify this partially-observed Markov process, we must implement both the process model (i.e., the unobserved process) and the measurement model (the observation process).
As we saw before, we would like to be able to:
\begin{enumerate}
\item \label{it:rproc} simulate from the process model, i.e., make a random draw from $X_{t+1}\,\vert\,X_{t}=x$ for arbitrary $x$ and $t$ (\code{rprocess}),
\item \label{it:dproc} compute the probability density function (pdf) of state transitions, i.e., compute $f(X_{t+1}=x'\,\vert\,X_{t}=x)$ for arbitrary $x$, $x'$, and $t$ (\code{dprocess}),
\item \label{it:rmeas} simulate from the measurement model, i.e., make a random draw from $Y_{t}\,\vert\,X_{t}=x$ for arbitrary $x$ and $t$ (\code{rmeasure}),
\item \label{it:dmeas} compute the measurement model pdf, i.e., $f(Y_{t}=y\,\vert\,X_{t}=x)$ for arbitrary $x$, $y$, and $t$ (\code{dmeasure}), and
\item \label{it:skel} compute the \emph{deterministic skeleton}.
  In discrete-time, this is the map $x\,\mapsto\,\mathbb{E}[X_{t+1}\,\vert\,X_{t}=x]$ for arbitrary $x$.
\end{enumerate}
For this simple model, all this is easy enough.
More generally, it will be difficult to do some of these things.
Depending on what we wish to accomplish, however, we may not need all of these capabilities and in particular,
\textbf{to use any particular one of the algorithms in \pomp, we need never specify \emph{all} of \ref{it:rproc}--\ref{it:skel}.}
For example, to simulate data, all we need is \ref{it:rproc} and \ref{it:rmeas}.
To run a particle filter (and hence to use iterated filtering, \code{mif}), one needs \ref{it:rproc} and \ref{it:dmeas}.
To do MCMC, one needs \ref{it:dproc} and \ref{it:dmeas}.
Nonlinear forecasting (\code{nlf}) and probe matching (\code{probe.match}) require \ref{it:rproc} and \ref{it:rmeas}.
Trajectory matching (\code{traj.match}) requires \ref{it:dmeas} and \ref{it:skel}.

Using \pomp, the first step is always to construct an \R\ object that encodes the model and the data.
Naturally enough, this object will be of class \pomp.
The key step in this is to specify functions to do some or all of \ref{it:rproc}--\ref{it:skel}, along with data and (optionally) other information.
The package provides a number algorithms for fitting the models to the data, for simulating the models, studying deterministic skeletons, and so on.
The documentation (\code{?pomp}) spells out the usage of the \pomp\ constructor, including detailed specifications for all its arguments and a worked example.

Let's see how to implement the Gompertz model in \pomp.
Here, we'll take the shortest path to this goal.
In the ``advanced topics in \pomp'' vignette, we show how one can make the codes much more efficient using compiled native (C or FORTRAN) code.

First, we write a function that implements the process model simulator.
This is a function that will simulate a single step ($t\to{t+{\Delta}t}$) of the unobserved process \eqref{eq:gompertz1}.
<<gompertz-proc-sim-def>>=
require(pomp)

gompertz.proc.sim <- function (x, t, params, delta.t, ...) {
  ## unpack the parameters:
  r <- params["r"]
  K <- params["K"]
  sigma <- params["sigma"]
  ## the state at time t:
  X <- x["X"]
  ## generate a log-normal random variable:
  eps <- exp(rnorm(n=1,mean=0,sd=sigma))
  ## compute the state at time t+1:
  S <- exp(-r*delta.t)
  xnew <- c(X=unname(K^(1-S)*X^S*eps))
  return(xnew)
}
@ 
The translation from the mathematical description \eqref{eq:gompertz1} to the simulator is straightforward.
When this function is called, the argument \code{x} contains the state at time \code{t}.
The parameters (including $K$, $r$, and $\sigma$) are passed in the argument \code{params}.
Notice that \code{x} and \code{params} are named numeric vectors and that the output must be also be a named numeric vector.
In fact, the names of the output vector (here \code{xnew}) must be the same as those of the input vector \code{x}.
The algorithms in \pomp\ all make heavy use of the \code{names} attributes of vectors and matrices.
The argument \code{delta.t} tells how big the time-step is. 
In this case, our time-step will be 1 unit; 
we'll see below how that gets specified.

Next, we'll implement a simulator for the observation process \eqref{eq:gompertz-obs}.
<<gompertz-meas-sim-def>>=
gompertz.meas.sim <- function (x, t, params, ...) {
  ## unpack the parameters:
  tau <- params["tau"]
  ## state at time t:
  X <- x["X"]
  ## generate a simulated observation:
  y <- c(Y=unname(rlnorm(n=1,meanlog=log(X),sd=tau)))
  return(y)
}
@ 
Again the translation from the model \eqref{eq:gompertz-obs} is straightforward.
When \code{gompertz.meas.sim} is called, the unobserved states at time \code{t} will be in the named numeric vector \code{x} and the parameters in \code{params} as before.
The function returns a named numeric vector that represents a single draw from the observation process \eqref{eq:gompertz-obs}.

Complementing the measurement model simulator is the corresponding measurement model density, which we can implement as follows:
<<gompertz-meas-dens-def>>=
gompertz.meas.dens <- function (y, x, t, params, log, ...) {
  ## unpack the parameters:
  tau <- params["tau"]
  ## state at time t:
  X <- x["X"]
  ## observation at time t:
  Y <- y["Y"]
  ## compute the likelihood of Y|X,tau
  f <- dlnorm(x=Y,meanlog=log(X),sdlog=tau,log=log)
  return(f)
}
@ 
We'll need this later on for likelihood-based inference.
Note that \code{gompertz.meas.dens} is closely related to \code{gompertz.meas.sim}.

\clearpage
\section{Simulating the model}

With the two functions above, we already have all we need to simulate the full model.
The first step is to construct an \R\ object of class \pomp\ which will serve as a container to hold the model and data.
This is done with a call to \pomp:
<<first-pomp-construction,eval=F>>=
gompertz <- pomp(
                 data=data.frame(
                   time=1:100,
                   Y=NA
                   ),
                 times="time",
                 rprocess=discrete.time.sim(
                   step.fun=gompertz.proc.sim,
                   delta.t=1
                   ),
                 rmeasure=gompertz.meas.sim,
                 t0=0
                 )
@ 
The first argument (\code{data}) specifies a data-frame that holds the data and the times at which the data were observed.
Since this is a toy problem, we have no data.
In a moment, however, we'll simulate some data so we can explore \pomp's various fitting methods.
The second argument (\code{times}) specifies which of the columns of \code{data} is the time variable.
The third argument (\code{rprocess}) specifies that the process model simulator will be in discrete time, one step at a time.
The function \code{discrete.time.sim} belongs to the \pomp\ package.
It takes the argument \code{step.fun}, which specifies the particular function that actually takes the step.
Its second argument, \code{delta.t}, specifies the duration of the time step (by default, \code{delta.t=1}).
The argument \code{rmeasure} specifies the measurement model simulator function.
\code{t0} fixes $t_0$ for this model; here we have chosen this to be one time unit before the first observation.

Before we can simulate the model, we need to settle on some parameter values.
We do this by specifying a named numeric vector that contains at least all the parameters needed by the functions \code{gompertz.proc.sim} and \code{gompertz.meas.sim}.
The parameter vector needs to specify the initial conditions $X(t_{0})=x_{0}$ as well.
<<set-params>>=
theta <- c(
           r=0.1,K=1,sigma=0.1,
           tau=0.1,
           X.0=1
           )
@ 
In addition to the parameters $r$, $K$, $\sigma$, and $\tau$, note that we've specified the initial condition $X.0$ in the vector \code{theta}.
The fact that the initial condition parameter's name ends in ``\code{.0}'' is significant: it tells \code{pomp} that this is the initial condition of the state variable \code{X}.
This use of the ``\code{.0}'' suffix is the default behavior of \pomp: 
one can also parameterize initial conditions in an arbitrary way using the optional \code{initializer} argument to \pomp.
See the documentation (\verb+?pomp+) for details.

Now we can simulate the model:
<<gompertz-first-simulation,eval=F>>=
gompertz <- simulate(gompertz,params=theta)
@ 
<<gompertz-get-data,eval=T,echo=F>>=
data(gompertz)
dat <- as.data.frame(gompertz)
gompertz <- pomp(
                 data=dat[c("time","Y")],
                 times="time",
                 rprocess=discrete.time.sim(
                   step.fun=gompertz.proc.sim,
                   delta.t=1
                   ),
                 rmeasure=gompertz.meas.sim,
                 t0=0
                 )
coef(gompertz) <- theta
@ 
Now \code{gompertz} is identical to what it was before, but the data that were there before have been replaced by simulated data.
The parameters (\code{theta}) at which the simulations were performed have also been saved internally to \code{gompertz}.
We can plot the simulated data via
<<eval=F>>=
plot(gompertz,variables="Y")
@ 
Fig.~\ref{fig:gompertz-first-simulation-plot} shows the results of this operation.

\begin{figure}
<<gompertz-plot,echo=F,fig=T>>=
plot(gompertz,variables=c("Y"))
@ 
\caption{
  Simulated data and unobserved states from the Gompertz model (Eqs.~\ref{eq:gompertz1}--\ref{eq:gompertz-obs}).
  This figure shows the output of the command \code{plot(gompertz,variables="Y")}.
}
\label{fig:gompertz-first-simulation-plot}
\end{figure}

\section{Computing likelihood using particle filtering}

Some parameter estimation algorithms in the \pomp\ package only require \code{rprocess} and \code{rmeasure}.
These include the nonlinear forecasting algorithm \code{nlf} and the probe-matching algorithm \code{probe.match}.
If we want to work with likelihood-based methods, however, we will need to be able to compute the likelihood of the data $Y_t$ given the states $X_t$.
Above, we wrote an \R\ function, \code{gompertz.meas.dens}, to do this.
We haven't yet used it all.
To do so, we'll need to incorporate it into the \pomp\ object.
We can do this by specifying the \code{dmeasure} argument in another call to \pomp:
<<second-pomp-construction>>=
dat <- as(gompertz,"data.frame")
theta <- coef(gompertz)
gompertz <- pomp(
                 data=dat[c("time","Y")],
                 times="time",
                 rprocess=discrete.time.sim(gompertz.proc.sim),
                 rmeasure=gompertz.meas.sim,
                 dmeasure=gompertz.meas.dens,
                 t0=0
                 )
coef(gompertz) <- theta
@ 
Note that we've first extracted the data from our old \code{gompertz} and set up the new one with the same data and parameters.
The calls to \code{coef} and \code{coef<-} in the lines above make sure the parameters have the same values they had before.

To compute the likelihood of the data, we can use the function \code{pfilter}.
This runs a plain vanilla particle filter (AKA sequential Monte Carlo) algorithm and results in an unbiased estimate of the likelihood.
See \citet{Arulampalam2002} for an excellent tutorial on particle filtering and \citet{Ionides2006} for a pseudocode description of the algorithm implemented in \pomp.
We must decide how many concurrent realizations (\emph{particles}) to use: the larger the number of particles, the smaller the Monte Carlo error but the greater the computational effort.
Let's run \code{pfilter} with 1000 particles to estimate the likelihood at the true parameters:
<<gompertz-pfilter-truth,eval=F>>=
pf <- pfilter(gompertz,params=theta,Np=1000)
loglik.truth <- logLik(pf)
loglik.truth
@ 
<<gompertz-pfilter-truth-eval,echo=F>>=
set.seed(457645443L)
<<gompertz-pfilter-truth>>
@ 
Since the true parameters (i.e., the parameters that generated the data) are stored within the \pomp\ object \code{gompertz} and can be extracted by the \code{coef} function, we could have done
<<gompertz-pfilter-truth-alt1,eval=F>>=
pf <- pfilter(gompertz,params=coef(gompertz),Np=1000)
@ 
or even just
<<gompertz-pfilter-truth-alt2,eval=F>>=
pf <- pfilter(gompertz,Np=1000)
@ 
which would have worked since the parameters are stored in the \pomp\ object \code{gompertz}.
Now let's compute the log likelihood at a different point in parameter space, one for which $r$, $K$, and $\sigma$ are 50\% higher than their true values.
<<gompertz-pfilter-guess,eval=F>>=
theta.true <- coef(gompertz)
theta.guess <- theta.true
theta.guess[c("r","K","sigma")] <- 1.5*theta.true[c("r","K","sigma")]
pf <- pfilter(gompertz,params=theta.guess,Np=1000)
loglik.guess <- logLik(pf)
@ 
<<gompertz-pfilter-guess-eval,echo=F>>=
set.seed(457645443L)
<<gompertz-pfilter-guess>>
@ 

\begin{textbox}
\caption{Implementation of the Kalman filter for the Gompertz model.}
\label{box:kalman-filter-def}
<<kalman-filter-def>>=
kalman.filter <- function (Y, X0, r, K, sigma, tau) {
  ntimes <- length(Y)
  sigma.sq <- sigma^2
  tau.sq <- tau^2
  cond.loglik <- numeric(ntimes)
  filter.mean <- numeric(ntimes)
  pred.mean <- numeric(ntimes)
  pred.var <- numeric(ntimes)
  m <- log(X0)
  v <- 0
  S <- exp(-r)
  for (k in seq_len(ntimes)) {
    pred.mean[k] <- M <- (1-S)*log(K) + S*m
    pred.var[k] <- V <- S*v*S+sigma.sq
    q <- V+tau.sq
    r <- log(Y[k])-M
    cond.loglik[k] <- dnorm(x=log(Y[k]),mean=M,sd=sqrt(q),log=TRUE)
    q <- 1/V+1/tau.sq
    filter.mean[k] <- m <- (log(Y[k])/tau.sq+M/V)/q
    v <- 1/q
  }
  list(
       pred.mean=pred.mean,
       pred.var=pred.var,
       filter.mean=filter.mean,
       cond.loglik=cond.loglik,
       loglik=sum(cond.loglik)
       )
}
@   
\end{textbox}

As we mentioned before, for this particular example, we can compute the likelihood exactly using the Kalman filter, using this as a check on the validity of the particle filtering algorithm.
An implementation of the Kalman filter is given in Box~\ref{box:kalman-filter-def}.
Let's run the Kalman filter on the example data we generated above:
<<kalman-filter-run>>=
y <- obs(gompertz)
x0 <- init.state(gompertz)
r <- coef(gompertz,"r")
K <- coef(gompertz,"K")
sigma <- coef(gompertz,"sigma")
tau <- coef(gompertz,"tau")
kf <- kalman.filter(y,x0,r,K,sigma,tau)
@ 
<<kalman-likelihood-correction,echo=F>>=
loglik.kalman <- kf$loglik-sum(log(obs(gompertz)))
@ 
In this case, the Kalman filter gives us a log likelihood of \Sexpr{round(loglik.kalman,2)}, 
while the particle filter with 1000 particles gives \Sexpr{round(loglik.truth,2)}.
Since the particle filter gives an unbiased estimate of the likelihood, the difference is due to Monte Carlo error in the particle filter.
One can reduce this error by using a larger number of particles and/or by re-running \code{pfilter} multiple times and averaging the resulting estimated likelihoods.
The latter approach has the advantage of allowing one to estimate the Monte Carlo error itself.

\clearpage
\section{Interlude: utility functions for extracting and changing pieces of a \pomp\ object}

The \pomp\ package provides a number of functions to extract or change pieces of a \pomp-class object.
%% Need references to S4 classes
One can read the documentation on all of these by doing \verb+class?pomp+ and \verb+methods?pomp+.
For example, as we've already seen, one can coerce a \pomp\ object to a data frame:
<<eval=F>>=
as(gompertz,"data.frame")
@ 
and if we \code{print} a \pomp\ object, the resulting data frame is what is shown, together with the call that created the \pomp\ object.
One has access to the data and the observation times using
<<eval=F>>=
obs(gompertz)
obs(gompertz,"Y")
time(gompertz)  
@ 
The observation times can be changed using
<<eval=F>>=
time(gompertz) <- 1:10
@ 
One can respectively view and change the zero-time by
<<eval=F>>=
timezero(gompertz)
timezero(gompertz) <- -10
@ 
and can respectively view and change the zero-time together with the observation times by doing, for example
<<eval=F>>=
time(gompertz,t0=TRUE)  
time(gompertz,t0=T) <- seq(from=0,to=10,by=1)
@ 
Alternatively, one can construct a new pomp object with the same model but with data restricted to a specified window:
<<eval=F>>=
window(gompertz,start=3,end=20)
@ 
Note that \code{window} does not change the zero-time.
One can display and modify model parameters using, e.g.,
<<eval=F>>=
coef(gompertz)
coef(gompertz,c("sigma","tau")) <- c(1,0)
@ 
Finally, one has access to the unobserved states via, e.g.,
<<eval=F>>=
states(gompertz)
states(gompertz,"X")
@ 

%% In the ``advanced_topics_in_pomp'' vignette, we show how one can get access to more of the underlying structure of a \pomp\ object.

\clearpage
\section{Transforming parameters}

The parameters in the Gompertz model above are constrained to be positive.
When we estimate these parameters using numerical search algorithms, we must find some way to ensure that these constraints will be honored.
A straightforward way to accomplish this is to transform the parameters so that they become unconstrained.
The following codes re-implement the Gompertz model using transformed parameters.

<<loggomp-proc-sim-def>>=
gompertz.proc.sim <- function (x, t, params, delta.t, ...) {
  ## unpack and untransform the parameters:
  r <- exp(params["log.r"])
  K <- exp(params["log.K"])
  sigma <- exp(params["log.sigma"])
  ## the state at time t:
  X <- x["X"]
  ## generate a log-normal random variable:
  eps <- exp(rnorm(n=1,mean=0,sd=sigma))
  ## compute the state at time t+1:
  S <- exp(-r*delta.t)
  xnew <- c(X=unname(K^(1-S)*X^S*eps))
  return(xnew)
}
@ 

<<loggomp-meas-sim-def>>=
gompertz.meas.sim <- function (x, t, params, ...) {
  ## unpack and untransform the parameters:
  tau <- exp(params["log.tau"])
  ## state at time t:
  X <- x["X"]
  ## generate a simulated observation:
  y <- c(Y=unname(rlnorm(n=1,meanlog=log(X),sdlog=tau)))
  return(y)
}
@ 

<<loggomp-meas-dens-def>>=
gompertz.meas.dens <- function (y, x, t, params, log, ...) {
  ## unpack and untransform the parameters:
  tau <- exp(params["log.tau"])
  ## state at time t:
  X <- x["X"]
  ## observation at time t:
  Y <- y["Y"]
  ## compute the likelihood of Y|X,tau
  f <- dlnorm(x=Y,meanlog=log(X),sdlog=tau,log=log)
  return(f)
}
@ 

Note that, in each of the above functions, we \emph{untransform} the parameters before we do any computations.

<<loggomp-pomp-construction,eval=F>>=
dat <- as(gompertz,"data.frame")
theta <- c(
           log(coef(gompertz,c("r","K","tau","sigma"))),
           coef(gompertz,"X.0")
           )
names(theta) <- c("log.r","log.K","log.tau","log.sigma","X.0")
gompertz <- pomp(
                 data=dat[c("time","Y")],
                 times="time",
                 rprocess=discrete.time.sim(gompertz.proc.sim),
                 rmeasure=gompertz.meas.sim,
                 dmeasure=gompertz.meas.dens,
                 t0=0
                 )
coef(gompertz) <- theta
@ 

A \code{pomp} object corresponding to the one just created (but with the \code{rprocess}, \code{rmeasure}, and \code{dmeasure} bits coded in C for speed) can be loaded by executing \verb+data(gompertz)+.

\clearpage
\section{Estimating parameters using iterated filtering: \code{mif}}

Iterated filtering is a technique for maximizing the likelihood obtained by filtering.
In \pomp, it is the particle filter that is iterated.
Iterated filtering is implemented in the \code{mif} function.
For a description of the algorithm and a description of its theoretical basis, see \citet{Ionides2006}.
A more complete set of proofs is provided in \citet{Ionides2011}.

The key idea of iterated filtering is to replace the model we are interested in fitting---which has time-invariant parameters---with a model that is just the same except that its parameters take a random walk in time.
As the intensity of this random walk approaches zero, the modified model approaches the original model.
Adding additional variability in this way has three positive effects:
\begin{inparaenum}[(i)]
\item it smooths the likelihood surface, which makes optimization easier, 
\item it combats \emph{particle depletion}, the fundamental difficulty associated with the particle filter, and
\item the additional variability can be exploited to estimate of the gradient of the (smoothed) likelihood surface \emph{with no more computation than is required to estimate of the value of the likelihood}.
\end{inparaenum}
Iterated filtering exploits these effects to optimize the likelihood in a computationally efficient manner.
As the filtering is iterated, the additional variability is decreased according to a \emph{cooling schedule}.
The cooling schedule can be adjusted in \code{mif}, as can the intensity of the parameter-space random walk and the other algorithm parameters.
See the documentation (\verb+?mif+) for details.

Let's use iterated filtering to obtain an approximate MLE for the data in \code{gompertz}.
We'll initialize the algorithm at several starting points around \code{theta.true} and just estimate the parameters $r$, $\tau$, and $\sigma$:
<<echo=F>>=
data(gompertz)
theta <- coef(gompertz)
theta.true <- theta
@ 
<<gompertz-multi-mif-calc,eval=F,echo=T>>=
estpars <- c("log.r","log.sigma","log.tau")
mf <- replicate(
                n=10,
                {
                  theta.guess <- theta.true
                  theta.guess[estpars] <- rnorm(
                                                n=length(estpars),
                                                mean=theta.guess[estpars],
                                                sd=1
                                                )
                  mif(
                      gompertz,
                      Nmif=100,
                      start=theta.guess,
                      pars=estpars,
                      rw.sd=c(
                        log.r=0.02,log.sigma=0.02,log.tau=0.05
                        ),
                      Np=2000,
                      var.factor=4,
                      ic.lag=10,
                      cooling.factor=0.999,
                      max.fail=10
                      )
                }
                )
##mf <- lapply(mf,continue,Nmif=50)
@ 

<<gompertz-post-mif,eval=F,echo=F>>=
theta.mif <- apply(sapply(mf,coef),1,mean)
loglik.mif <- replicate(n=10,logLik(pfilter(mf[[1]],params=theta.mif,Np=10000)))
loglik.mif.sd <- sd(loglik.mif)
bl <- mean(loglik.mif)
loglik.mif <- bl+log(mean(exp(-bl+loglik.mif)))
loglik.true <- replicate(n=10,logLik(pfilter(gompertz,params=theta.true,Np=10000)))
loglik.true.sd <- sd(loglik.true)
loglik.true <- bl+log(mean(exp(-bl+loglik.true)))
@ 

<<gompertz-multi-mif-eval,echo=F>>=
set.seed(334388458L)
binary.file <- "gompertz-multi-mif.rda"
if (file.exists(binary.file)) {
  load(binary.file)
} else {
  tic <- Sys.time()
<<gompertz-multi-mif-calc>>
<<gompertz-post-mif>>
  toc <- Sys.time()
  etime <- toc-tic
  save(mf,estpars,theta.mif,theta.true,loglik.mif,loglik.true,loglik.mif.sd,loglik.true.sd,etime,file=binary.file)
}
cbind(
#      guess=c(signif(theta.guess[estpars],3),loglik=round(loglik.guess,1)),
      mle=c(signif(theta.mif[estpars],3),loglik=round(loglik.mif,1),loglik.sd=round(loglik.mif.sd,1)),
      truth=c(signif(theta.true[estpars],3),loglik=round(loglik.true,1),loglik.sd=round(loglik.true.sd,1))
      ) -> results.table
@ 

Each of the \Sexpr{length(mf)} \code{mif} runs ends up at a different place.
In this case (but by no means in every case), we can average across these parameter estimates to get an approximate maximum likelihood estimate.
We'll evaluate the likelihood several times at this estimate to get an idea of the Monte Carlo error in our likelihood estimate.
<<eval=F>>=
<<gompertz-post-mif>>
@ 

<<multi-mif-plot,echo=F,eval=F>>=
op <- par(mfrow=c(4,1),mar=c(3,3,0,0),mgp=c(2,1,0),bty='l')
loglik <- sapply(mf,function(x)conv.rec(x,"loglik"))
log.r <- sapply(mf,function(x)conv.rec(x,"log.r"))
log.sigma <- sapply(mf,function(x)conv.rec(x,"log.sigma"))
log.tau <- sapply(mf,function(x)conv.rec(x,"log.tau"))
matplot(loglik,type='l',lty=1,xlab="",ylab=expression(log~L),xaxt='n')
matplot(log.r,type='l',lty=1,xlab="",ylab=expression(log~r),xaxt='n')
matplot(log.sigma,type='l',lty=1,xlab="",ylab=expression(log~sigma),xaxt='n')
matplot(log.tau,type='l',lty=1,xlab="MIF iteration",ylab=expression(log~tau))
par(op)
@ 


\begin{figure}
<<mif-plot,fig=T,echo=F>>=
<<multi-mif-plot>>
@ 
\caption{
  Convergence plots can be used to help diagnose convergence of the iterated filtering algorithm.
  This shows part of the output of \code{compare.mif(mf)}.
}
\label{fig:convplot}
\end{figure}

<<first-mif-results-table,echo=F>>=
print(results.table)
@ 

\clearpage
\section{Trajectory matching: \code{traj.match}}

The idea behind trajectory matching is a simple one.
One attempts to fit a deterministic dynamical trajectory to the data.
This is tantamount to assuming that all the stochasticity in the system is in the measurement process.
In \pomp, the trajectory is computed using the \code{trajectory} function, which in turn uses the \code{skeleton} slot of the \pomp\ object.
The \code{skeleton} slot should be filled with the deterministic skeleton of the process model.
In the discrete-time case, this is the map
\begin{equation*}
  x\,\mapsto\,\expect{X_{t+1}\;\vert\;X_{t}=x,\theta}.
\end{equation*}
In the continuous-time case, this is the vectorfield
\begin{equation*}
  x\,\mapsto\,\lim_{{\Delta}{t}\,\to\,0}\,\expect{\frac{X_{t+{\Delta}{t}}-x}{{\Delta}{t}}\;\Big{\vert}\;X_{t}=x,\theta}.
\end{equation*}
Our discrete-time Gompertz has the deterministic skeleton
\begin{equation}\label{eq:gompertz-skel}
  x\,\mapsto\,K^{1-S}\,x^{S},
\end{equation}
where $S=e^{-r\,{\Delta}t}$ and ${\Delta}t$ is the time-step.
This can be implemented in the \R\ function
<<gompertz-skeleton-def,echo=T>>=
gompertz.skel <- function (x, t, params, ...) {
  delta.t <- 1
  r <- exp(params["log.r"])
  K <- exp(params["log.K"])
  X <- x["X"]
  S <- exp(-r*delta.t)
  xnew <- c(X=unname(K^(1-S)*X^S))
  return(xnew)
}
@ 

We can incorporate the deterministic skeleton into a new \pomp\ object via the \code{skeleton.map} argument:
<<new-gompertz-pomp-construction,echo=T>>=
new.gompertz <- pomp(
                     data=data.frame(time=1:200,Y=NA),
                     times="time",
                     rprocess=discrete.time.sim(gompertz.proc.sim),
                     rmeasure=gompertz.meas.sim,
                     dmeasure=gompertz.meas.dens,
                     skeleton.map=gompertz.skel,
                     t0=0
                     )
coef(new.gompertz) <- theta
coef(new.gompertz,"X.0") <- 0.1
coef(new.gompertz,"log.r") <- log(0.1)
coef(new.gompertz,"log.tau") <- log(0.05)
coef(new.gompertz,"log.sigma") <- log(0.05)
new.gompertz <- simulate(new.gompertz,seed=88737400L)
@ 
We use the \code{skeleton.map} argument for discrete-time processes and \code{skeleton.vectorfield} for continuous-time processes.
%% Note that we have turned off the process noise in \code{new.gompertz} (next to last line) so that trajectory matching is actually formally appropriate for this model.

The \pomp\ function \code{traj.match} calls the optimizer \code{optim} to minimize the discrepancy between the trajectory and the data.
The discrepancy is measured using the \code{dmeasure} function from the \pomp\ object.
Fig.~\ref{fig:trajmatch-plot} shows the results of this fit.
<<gompertz-trajmatch-calc,eval=F>>=
tm <- traj.match(
                 new.gompertz,
                 start=coef(new.gompertz),
                 est=c("log.r","log.K","log.tau","X.0"),
                 method="Nelder-Mead",
                 maxit=1000,
                 reltol=1e-8
                 )
@ 
<<gompertz-trajmatch-eval,echo=F,eval=T>>=
binary.file <- "gompertz-trajmatch.rda"
if (file.exists(binary.file)) {
  load(binary.file)
} else {
<<gompertz-trajmatch-calc>>
  save(new.gompertz,tm,file=binary.file)
}
@

\begin{figure}
<<trajmatch-plot,fig=T,echo=F,eval=T>>=
op <- par(mfrow=c(1,1),mar=c(3,3,0,0),mgp=c(2,1,0),bty='l')
plot(time(tm),obs(tm,"Y"),xlab="time",ylab=expression(X,Y),type='o')
lines(time(tm),states(tm,"X"),lwd=2)
par(op)
@ 
\caption{
  Illustration of trajectory matching.
  The points show data simulated from \code{new.gompertz}.
  The solid line shows the trajectory of the best-fitting model, obtained using \code{traj.match}.
  Fitting by trajectory matching is tantamount to the assumption that the data-generating process has no process noise but only measurement error.
}
\label{fig:trajmatch-plot}
\end{figure}

\clearpage
\section{Probe matching: \code{probe.match}}

In probe matching, we fit a model to data using a set of summary statistics.
We evaluate these statistics on the data and compare them to the distribution of values they take on simulations, then adjust model parameters to maximize agreement between model and data according to some criterion.
Following \citet{Kendall1999}, we refer to the summary statistics as \emph{probes}.
In probe-matching, one has unrestricted choice of probes, and there are a great many probes that one might sensibly choose.
This introduces a degree of subjectivity into the inference procedure but has the advantage of allowing the investigator to identify \emph{a priori} those features of a data set he or she believes to be informative.

In this section, we'll illustrate probe matching using a stochastic version of the Ricker map.
In this discrete-time model, $N_t$ represents the (true) size of a population at time $t$ and obeys
\begin{equation*}
  N_{t+1}=r\,N_t\,\exp(-N_t+e_t),\qquad e_t\!\sim\!\mathrm{normal}(0,\sigma).
\end{equation*}
In addition, we assume that measurements $y_t$ of $N_t$ are themselves noisy, according to
\begin{equation*}
  y_t\!\sim\!\mathrm{Poisson}(\phi\,N_t).
\end{equation*}
As before, we'll begin by writing an \R\ function that implements a simulator (\code{rprocess}) for the Ricker model.
It will be convenient to work with log-transformed parameters $\log r$, $\log\sigma$, $\log\phi$.
Thus
<<ricker-map-defn>>=
ricker.sim <- function (x, t, params, delta.t, ...) {
  e <- rnorm(n=1,mean=0,sd=exp(params["log.sigma"])) 
  xnew <- c(
            exp(params["log.r"]+log(x["N"])-x["N"]+e),
            e
            )
  names(xnew) <- c("N","e")
  xnew
}
@
Note that, in this implementation, $e$ is taken to be a state variable.
This is not strictly necessary, but it might prove useful, for example, in \emph{a posteriori} diagnostic checking of model residuals.
Now we can construct a \code{pomp} object; in this case, we use the \code{discrete.time.sim} plug-in.
Note how we specify the measurement model.
<<ricker-pomp>>=
ricker <- pomp(
               data=data.frame(time=seq(0,50,by=1),y=NA),
               times="time",
               t0=0,
               rprocess=discrete.time.sim(
                 step.fun=ricker.sim
                 ),
               measurement.model=y~pois(lambda=N*exp(log.phi))
               )
coef(ricker) <- c(
                  log.r=3.8,
                  log.sigma=log(0.3),
                  log.phi=log(10),
                  N.0=7,
                  e.0=0
                  )
ricker <- simulate(ricker,seed=73691676L)
@ 

A pre-built \code{pomp} object implementing this model is included with the package.
Its \code{rprocess}, \code{rmeasure}, and \code{dmeasure} components are written in C and are thus a bit faster than the \R\ implementation above.
Do 
<<get-ricker,echo=T,eval=T>>=
data(ricker)
@ 
to load this \code{pomp} object.

In \pomp, probes are simply functions that can be applied to an array of real or simulated data to yield a scalar or vector quantity.
Several functions that create commonly-useful probes are included with the package.
Do \verb+?basic.probes+ to read the documentation for these probes.
In this illustration, we will make use of several probes recommended by \citet{Wood2010}: \code{probe.marginal}, \code{probe.acf}, and \code{probe.nlar}.
\code{probe.marginal} regresses the data against a sample from a reference distribution; 
the probe's values are those of the regression coefficients.
\code{probe.acf} computes the auto-correlation or auto-covariance of the data at specified lags.
\code{probe.nlar} fits a simple nonlinear (polynomial) autoregressive model to the data;
again, the coefficients of the fitted model are the probe's values.
We construct our set of probes by specifying a list
<<probe-list>>=
plist <- list(
              probe.marginal("y",ref=obs(ricker),transform=sqrt),
              probe.acf("y",lags=c(0,1,2,3,4),transform=sqrt),
              probe.nlar("y",lags=c(1,1,1,2),powers=c(1,2,3,1),transform=sqrt)
              )
@ 
An examination of the structure of \code{plist} reveals that it is a list of functions of a single argument.
Each of these functions can be applied to the \code{ricker}'s data or to simulated data sets.
A call to \pomp's function \code{probe} results in the application of these functions to the data, their application to each of some large number, \code{nsim}, of simulated data sets, and finally to a comparison of the two.
To see this, we'll apply probe to the Ricker model at the true parameters and at a wild guess.
<<first-probe>>=
pb.truth <- probe(ricker,probes=plist,nsim=1000,seed=1066L)
guess <- c(log.r=log(20),log.sigma=log(1),log.phi=log(20),N.0=7,e.0=0)
pb.guess <- probe(ricker,params=guess,probes=plist,nsim=1000,seed=1066L)
@ 
Results summaries and diagnostic plots showing the model-data agreement and correlations among the probes can be obtained by 
<<first-probe-plot,eval=F>>=
summary(pb.truth)
summary(pb.guess)
plot(pb.truth)
plot(pb.guess)
@ 
An example of a diagnostic plot (using a simplified set of probes) is shown in Fig.~\ref{fig:ricker-probe-plot}.
Among the quantities returned by \code{summary} is the synthetic likelihood \citep{Wood2010}.
It is this synthetic likelihood that \pomp\ attempts to maximize in probe matching.

\begin{figure}
<<ricker-probe-plot,echo=F,fig=T>>=
  pb <- probe(ricker,
              probes=list(
                probe.marginal("y",ref=obs(ricker),transform=sqrt),
                probe.acf("y",lags=c(0,1,3),transform=sqrt),
                mean=probe.mean("y",transform=sqrt)
                       ),
              nsim=1000,
              seed=1066L
              )
  plot(pb)
@ 
\caption{
  Results of \code{plot} on a \code{probed.pomp}-class object.
  Above the diagonal, the pairwise scatterplots show the values of the probes on each of \Sexpr{summary(pb)$nsim} data sets.
  The red lines show the values of each of the probes on the data.
  The panels along the diagonal show the distributions of the probes on the simulated data, together with their values on the data and a two-sided p-value.
  The numbers below the diagonal indicate the Pearson correlations between the corresponding probes.
}
\label{fig:ricker-probe-plot}
\end{figure}

Let us now attempt to fit the Ricker model to the data using probe-matching.
<<ricker-probe-match-calc,eval=F>>=
pm <- probe.match(
                  pb.guess,
                  est=c("log.r","log.sigma","log.phi"),
                  method="Nelder-Mead",
                  maxit=2000,
                  seed=1066L,
                  reltol=1e-8,
                  trace=3
                  )
summary(pm)
@ 
This code runs a Nelder-Mead optimizer from the starting parameters \code{guess} in an attempt to maximize the synthetic likelihood based on the probes in \code{plist}.
Both the starting parameters and the probes are stored internally in \code{pb.guess}, which is why we don't specify them explicitly here;
if we wanted to change these, we could do so by specifying the \code{params} and/or \code{probes} arguments to \code{probe.match}.
See \code{?probe.match} for full documentation.

<<ricker-probe.match-eval,echo=F,eval=T,results=hide>>=
binary.file <- "ricker-probe-match.rda"
if (file.exists(binary.file)) {
  load(binary.file)
} else {
<<ricker-probe-match-calc>>
  save(pm,file=binary.file)
}
@

By way of putting the synthetic likelihood in context, let's compare the results of estimating the Ricker model parameters using probe-matching and using iterated filtering, which is based on likelihood.
The following code runs 600 MIF iterations starting at \code{guess}:
<<ricker-mif-calc,eval=F>>=
mf <- mif(
          ricker,
          start=guess,
          Nmif=100,
          Np=1000,
          cooling.factor=0.99,
          var.factor=2,
          ic.lag=3,
          max.fail=50,
          rw.sd=c(log.r=0.1,log.sigma=0.1,log.phi=0.1)
          )
mf <- continue(mf,Nmif=500,max.fail=20)
@ 

<<ricker-mif-eval,echo=F,eval=T,results=hide>>=
binary.file <- "ricker-mif.rda"
if (file.exists(binary.file)) {
  load(binary.file)
} else {
<<ricker-mif-calc>>
  save(mf,file=binary.file)
}
@
The following code compares parameters, likelihoods, and synthetic likelihoods (based on the probes in \code{plist}) at each of 
\begin{inparaenum}
\item the wild guess,
\item the truth,
\item the MLE from \code{mif}, and
\item the maximum synthetic likelihood estimate from \code{probe.match}.
\end{inparaenum}
<<>>=
pf.truth <- pfilter(ricker,Np=1000,max.fail=50,seed=1066L)
pf.guess <- pfilter(ricker,params=guess,Np=1000,max.fail=50,seed=1066L)
pf.mf <- pfilter(mf,Np=1000,seed=1066L)
pf.pm <- pfilter(pm,Np=1000,max.fail=10,seed=1066L)
pb.mf <- probe(mf,nsim=1000,probes=plist,seed=1066L)
res <- rbind(
             cbind(guess=guess,truth=coef(ricker),MLE=coef(mf),PM=coef(pm)),
             loglik=c(
               pf.guess$loglik,
               pf.truth$loglik,
               pf.mf$loglik,
               pf.pm$loglik
               ),
             synth.loglik=c(
               summary(pb.guess)$synth.loglik,
               summary(pb.truth)$synth.loglik,
               summary(pb.mf)$synth.loglik,
               summary(pm)$synth.loglik
               )
             )

print(res,digits=3)
@ 

\clearpage
\section{Nonlinear forecasting: \code{nlf}}

\citet{Ellner1998,Kendall1999,Kendall2005}.  To be added.

<<first-nlf,echo=F,eval=F>>=
estnames <- c('alpha.2','alpha.3','tau')
out <- nlf(
           gompertz,
           start=theta.guess,
           nasymp=2000,
           est=estnames,
           lags=c(4,6),
           seed=5669345L,
           skip.se=TRUE,
           method="Nelder-Mead",
           trace=0,
           maxit=100,
           reltol=1e-8,
           transform=function(x)x,
           eval.only=FALSE
           )
@ 
<<first-nlf-results,echo=F,eval=F>>=
print(
      cbind(
            guess=theta.guess[estnames],
            fit=out$params[estnames],
            truth=theta.true[estnames]
            ),
      digits=3
      )
@ 

\section{Bayesian sequential Monte Carlo: \code{bsmc}}

\citet{Liu2001b}.  To be added.

\section{Particle Markov chain Monte Carlo: \code{pmcmc}}

\citet{Andrieu2010}.  To be added.

\clearpage
\section{A more complex example: a seasonal epidemic model}

The SIR model is a mainstay of theoretical epidemiology.
It has the deterministic skeleton
\begin{equation*}
  \begin{aligned}
    &\frac{dS}{dt}=\mu\,(N-S)-\beta(t)\,\frac{I}{N}\,S\\
    &\frac{dI}{dt}=\beta(t)\,\frac{I}{N}\,S-\gamma\,I-\mu\,I\\
    &\frac{dR}{dt}=\gamma\,I-\mu\,R\\
  \end{aligned}
\end{equation*}
Here $N=S+I+R$ is the (constant) population size and $\beta$ is a time-dependent contact rate.
We'll assume that the contact rate is periodic and implement it as a covariate.
As an additional wrinkle, we'll assume that the rate of the infection process $\beta\,I/N$ is perturbed by white noise.

As in the earlier example, we need to write a function that will simulate the process.
We can use \code{gillespie.sim} to implement this using the exact stochastic simulation algorithm of \citet{Gillespie1977a}.
This will be quite slow and inefficient, however, so we'll use the so-called ``tau-leap'' algorithm, one version of which is implemented in \pomp\ using Euler-multinomial processes.
Before we do this, we'll first define the basis functions that will be used for the seasonality.
It is convenient to use periodic B-splines for this purpose.
The following codes set up this basis.
<<seas-basis>>=
tbasis <- seq(0,25,by=1/52)
basis <- periodic.bspline.basis(tbasis,nbasis=3)
colnames(basis) <- paste("seas",1:3,sep='')
@ 

Now we'll define the process model simulator.
Since we have covariates now, our function will have one additional argument, \code{covars}, which will contain the value of each covariate at time \code{t}, established using linear interpolation if necessary.
<<sir-proc-sim-def>>=
sir.proc.sim <- function (x, t, params, covars, delta.t, ...) {
  params <- exp(params)
  with(
       as.list(c(x,params,covars)),
       {
         beta <- exp(sum(log(c(beta1,beta2,beta3))*c(seas1,seas2,seas3)))
         beta.var <- beta.sd^2
         dW <- if (beta.var>0) 
           rgamma(n=1,shape=delta.t/beta.var,scale=beta.var) 
         else 
           delta.t
         foi <- (iota+beta*I*dW/delta.t)/pop
         trans <- c(
                    rpois(n=1,lambda=mu*pop*delta.t),
                    reulermultinom(n=1,size=S,rate=c(foi,mu),dt=delta.t),
                    reulermultinom(n=1,size=I,rate=c(gamma,mu),dt=delta.t),
                    reulermultinom(n=1,size=R,rate=c(mu),dt=delta.t)
                    )
         c(
           S=S+trans[1]-trans[2]-trans[3],
           I=I+trans[2]-trans[4]-trans[5],
           R=R+trans[4]-trans[6],
           cases=cases+trans[4],
           W=if (beta.sd>0) W+(dW-delta.t)/beta.sd else W
           )
       }
       )
}
@ 
Let's look at this definition in a bit of detail.
We will be log-transforming the parameters: the first line untransforms them.
Here, we use \code{with} to make the codes a bit easier to read.
The variable \code{beta} will be the transmission rate: 
the time-dependence of this rate is parameterized using the basis functions, the current values have been passed via the \code{covars} argument.
The next lines make a draw, \code{dW}, from a Gamma random variable which will model environmental stochasticity as white noise in the transmission process \citep{Breto2009,He2010}.
The next line computes the force of infection, \code{foi}.
\code{trans} is next filled with random draws of all the transitions between the S, I, and R compartments.
Births are modeled using a Poisson distribution, the number of births is stored in \code{trans[1]}. 
Individuals leave the S class through either death or infection: \code{trans[2]} will contain the number infected, \code{trans[3]} the number dead.
The number leaving the I and R classes are handled similarly.
See the documentation on \code{reulermultinom} for a more thorough explanation of how this function works.
Finally, a named vector is returned that contains the new values of the state variables, each of which is the old value, adjusted by the transitions.
Note that the state variable \code{cases} accumulates the number of I$\to$R transitions and \code{W} accumulates the (standardized) white noise.

Now we're ready to construct the \pomp\ object.
<<sir-pomp-def>>=
pomp(
     data=data.frame(
       time=seq(1/52,4,by=1/52),
       reports=NA
       ),
     times="time",
     t0=0,
     tcovar=tbasis,
     covar=basis,
     rprocess=euler.sim(
       step.fun=sir.proc.sim,
       delta.t=1/52/20
       ),
     measurement.model=reports~binom(size=cases,prob=exp(rho)),
     zeronames=c("cases"),
     initializer=function(params, t0, comp.names, ...){
       p <- exp(params)
       snames <- c("S","I","R","cases","W")
       fracs <- p[paste(comp.names,"0",sep=".")]
       x0 <- numeric(length(snames))
       names(x0) <- snames
       x0[comp.names] <- round(p['pop']*fracs/sum(fracs))
       x0
     },
     comp.names=c("S","I","R")
     ) -> sir
@ 
The specification of \code{data}, \code{times}, and \code{t0} should be familiar.
The covariates are specified using the arguments \code{tcovar} and \code{covar}.
We use \code{euler.sim} to specify the process simulator (\code{rprocess}).
We are approximating the continuous-time process using an Euler simulator with a time-step of 1/20 of a week.
Both \code{rmeasure} and \code{dmeasure} can be specified at once using the \code{measurement.model} argument.
Here, we model the observation process using a binomial process, where the \emph{reporting rate}, \code{rho}, is the probability that a case is reported.

As we noted before, the state variable \code{cases} accumulates the number of cases (i.e., the number of I$\to$R transitions).
However, the data are not the cumulative number of cases, but the number of cases that have occurred since the last report.
Specifying \code{cases} in the \code{zeronames} argument has the effect of re-setting the state variable \code{cases} to zero after each observation.

Finally, in this example, we do not use the default parameterization of the initial states.
Instead, we specify a custom \code{initializer} argument.
We want instead to parameterize the initial states in terms of the fractions of the total population contained in each compartment.
In particular, as we see in the \code{initializer} argument to \pomp, we normalize so that the sum of \code{S.0}, \code{I.0}, and \code{R.0} is 1, then multiply by the initial population size, and then round to the nearest whole number.
Note that the initializer we have specified needs an argument \code{comp.names} (the names of the S, I, and R state variables).
This is set in the last line.
More generally, one can give any number or kind of additional arguments to \pomp:
they will be passed to the \code{initializer}, \code{rprocess}, \code{dprocess}, \code{rmeasure}, \code{dmeasure}, and \code{skeleton} functions, if these exist.
This feature aids in the writing of customized \pomp\ objects.

Now we'll simulate data using the parameters
<<sir-params>>=
theta <- c(
           gamma=26,mu=1/50,iota=10,
           beta1=1200,beta2=1500,beta3=900,
           beta.sd=1e-2,
           pop=5e5,
           rho=0.6,
           S.0=26/1200,I.0=0.001,R.0=1-0.001-26/1200
           )
@ 
<<sir-first-sim>>=
sir <- simulate(sir,nsim=1,params=log(theta),seed=329348545L)
@ 
Figure~\ref{fig:sir-plot} shows the simulated data and state variable trajectories.

\begin{figure}
<<sir-plot,fig=T,echo=F>>=
data(euler.sir)
sir <- simulate(euler.sir,nsim=1,params=c(log(theta),nbasis=3,period=1,degree=3),seed=329348545L)
plot(sir)
@   
  \caption{Results of \code{plot(sir)}.}
  \label{fig:sir-plot}
\end{figure}

\clearpage
\bibliographystyle{fullnat}
\bibliography{pomp}

\end{document}

<<restore-opts,echo=F,results=hide>>=
options(glop)
@ 
