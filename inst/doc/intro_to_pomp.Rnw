\documentclass[10pt,reqno,final]{amsart}
%\VignetteIndexEntry{Introduction to pomp by example}
\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{natbib}

\setlength{\textwidth}{6.25in}
\setlength{\textheight}{8.75in}
\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{-.35in}
\setlength{\parskip}{.1in}  
\setlength{\parindent}{0.0in}  
\setcounter{secnumdepth}{1}
\setcounter{tocdepth}{1}

\newcommand\code[1]{\texttt{#1}}
\newcommand{\R}{\textsf{R}}

\title[Introduction to \texttt{pomp}]{Introduction to \texttt{pomp} by example}

\author[A. A. King]{Aaron A. King}

\address{A. A. King, Departments of Ecology \& Evolutionary Biology and Mathematics, University of Michigan, Ann Arbor, Michigan 48109-1048 USA}

\email{kingaa@umich.edu} 

\urladdr{http://www.umich.edu/\~{}kingaa}

%% \date{\today}

\begin{document}

\SweaveOpts{echo=T,results=verbatim,print=F,eps=F,pdf=T,keep.source=T}

\maketitle

\tableofcontents

<<echo=F,results=hide>>=
  glop <- options(keep.source=TRUE,width=60,continue=" ",prompt=" ")
  library(pomp)
  set.seed(5384959)
@ 

\section{A first example: the two-dimensional Ornstein-Uhlenbeck process.}

To begin with, for simplicity, we will study a discrete-time process.
Later we'll look at a continuous-time model.
The \code{pomp} package is designed with continuous-time processes in mind, but the associated computational effort is typically greater, and the additional complexities are best postponed until the structure and usage of the package is understood.
The unobserved Ornstein-Uhlenbeck (OU) process $X_{t}\in\mathbb{R}^2$ satisfies
\begin{equation*}
  X_{t} = A\,X_{t-1}+\xi_{t}.
\end{equation*}
The observation process is
\begin{equation*}
  Y_{t} = B\,X_{t}+\varepsilon_{t}.
\end{equation*}
In these equations, $A$ and and $B$ are 2$\times$2 constant matrices; $\xi_{t}$ and $\varepsilon_{t}$ are mutually-independent families of i.i.d.\ bivariate normal random variables.
We let $\sigma\sigma^T$ be the variance-covariance matrix of $\xi_{t}$, where $\sigma$ is lower-triangular;
likewise, we let $\tau\tau^T$ be that of $\varepsilon_{t}$.

\subsection{Defining a partially observed Markov process.}

In order to fully specify this partially-observed Markov process, we must implement both the process model (i.e., the unobserved process) and the measurement model (the observation process).
That is, we would like to be able to:
\begin{enumerate}
\item \label{it:rproc} simulate from the process model, i.e., make a random draw from $X_{t+1}\,\vert\,X_{t}=x$ for arbitrary $x$ and $t$,
\item \label{it:dproc} compute the probability density function (pdf) of state transitions, i.e., compute $f(X_{t+1}=x'\,\vert\,X_{t}=x)$ for arbitrary $x$, $x'$, and $t$,
\item \label{it:rmeas} simulate from the measurement model, i.e., make a random draw from $Y_{t}\,\vert\,X_{t}=x$ for arbitrary $x$ and $t$, and
\item \label{it:dmeas} compute the measurement model pdf, i.e., $f(Y_{t}=y\,\vert\,X_{t}=x)$ for arbitrary $x$, $y$, and $t$.
\end{enumerate}
For this simple model, all this is easy enough.
In general, it will be difficult to do some of these things.
Depending on what we wish to accomplish, however, we may not need all of these capabilities.
For example, to simulate data, all we need is \ref{it:rproc} and \ref{it:rmeas}.
To run a particle filter (and hence to use iterated filtering, \code{mif}), one needs \ref{it:rproc} and \ref{it:dmeas}.
To do MCMC, one needs \ref{it:dproc} and \ref{it:dmeas}.
Nonlinear forecasting (\code{nlf}) requires \ref{it:rproc} and \ref{it:rmeas}.
In \code{pomp}, one constructs an object of class \code{pomp} by specifying functions to do some or all of \ref{it:rproc}--\ref{it:dmeas}, along with data and other information.
The package provides algorithms for fitting the models to the data, for simulating the models, studying deterministic skeletons, and so on.
The documentation (\code{?pomp}) spells out the usage of the \code{pomp} constructor, including detailed specifications for all its arguments and a worked example.

\subsection{Building the \code{pomp} object}

We build a \code{pomp} object by specifying some or all of the four basic elements mentioned above.
We'll go through this in some detail here, writing each of these functions from scratch.
Later, we'll look at some shortcuts that the package provides to streamline the process.

First, we write a function that implements the process model simulator.
In this function, we assume that:
\begin{enumerate}
\item \code{xstart} will be a matrix, each column of which is a vector of initial values of the state process;
\item \code{params} will be a matrix, the columns of which are parameter vectors;
\item \code{times} will be a vector of times at which realizations of the state process are required.  In particular, \code{times[1]} is the initial time (corresponding to \code{xstart}).
\end{enumerate}
<<>>=
  ou2.rprocess <- function (xstart, times, params, ...) { 
    ## this function simulates two discrete-time OU processes
    nreps <- ncol(xstart)
    ntimes <- length(times)
    x <- array(0,dim=c(2,nreps,ntimes))
    rownames(x) <- rownames(xstart)
    x[,,1] <- xstart
    for (k in 2:ntimes) {
      for (j in 1:nreps) {
        xi <- rnorm(2,mean=0,sd=1)
        x['x1',j,k] <- params['alpha.1',j]*x['x1',j,k-1]+
                         params['alpha.3',j]*x['x2',j,k-1]+
                         params['sigma.1',j]*xi[1]
        x['x2',j,k] <- params['alpha.2',j]*x['x1',j,k-1]+
                         params['alpha.4',j]*x['x2',j,k-1]+
                         params['sigma.2',j]*xi[1]+
                         params['sigma.3',j]*xi[2]
      }
    }
    x
  }
@ 
Notice that this function returns a rank-3 array (\code{x}), which has the realized values of the state process at the requested times.
Notice too that \code{x} has rownames.
When this function is called, in the course of any algorithm that uses it, some basic error checks will be performed.

Next, we write a function that computes the likelihoods of a set of process model state transitions.
Critically, in writing this function, we are allowed to assume that the transition from \verb+x[,j,k]+ to \verb!x[,j,k+1]! is elementary, i.e., that no transition has occurred between times \verb+times[k]+ and \verb!times[k+1]!.
If we weren't allowed to make this assumption, it would be very difficult indeed to write the transition probabilities.
Indeed, were we able to do so, we'd have no need for \code{pomp}!
<<>>=
  ou2.dprocess <- function (x, times, params, log, ...) { 
    ## this function simulates two discrete-time OU processes
    nreps <- ncol(x)
    ntimes <- length(times)
    xi.scal <- numeric(2)
    f <- array(0,dim=c(nreps,ntimes-1))
    for (k in 2:ntimes) {
      for (j in 1:nreps) {
        xi.scal[1] <- x['x1',j,k]-params['alpha.1',j]*x['x1',j,k-1]-
                        params['alpha.3',j]*x['x2',j,k-1]
        xi.scal[2] <- x['x2',j,k]-params['alpha.2',j]*x['x1',j,k-1]-
                        params['alpha.4',j]*x['x2',j,k-1]-
                        params['sigma.2',j]/params['sigma.1',j]*xi.scal[1]
        f[j,k-1] <- sum(
                        dnorm(
                              x=xi.scal,
                              mean=0,
                              sd=params[c("sigma.1","sigma.3"),j],
                              log=TRUE
                              ),
                        na.rm=T
                        )
      }
    }
    if (log) f else exp(f)
  }
@ 
In this function, \code{times} and \code{params} are as before, and \code{x} is a rank-3 array.
Notice that \code{dprocess} returns a rank-2 array (\code{f}) with dimensions \code{ncol(params)}$times$\code{(length(times)-1)}.
Each row of \code{f} corresponds to a different parameter point; each column corresponds to a different transition.
You can think of \code{x} as an array that might have been generated by a call to the \code{rprocess} function above.
\begin{textbf}
[NB: At present, \code{pomp} has no methods for fitting models to data that make use of \ref{it:dproc}.
  There is therefore at present no reason to specify \code{dprocess} in practice.
  The example above is given purely for illustrative purposes.]
\end{textbf}

Third, we write a measurement model simulator.
In this function, \code{x}, \code{t}, and \code{params} are states, time, and parameters, but they have a different form from those above.
In particular, \code{x} and \code{params} are vectors.
Notice that we give the returned vector, \code{y}, names to match the names of the data.
<<>>=
  bvnorm.rmeasure <- function (x, t, params, ...) {
    ## noisy observations of the two walks with common noise SD 'tau'
    c(
      y1=rnorm(n=1,mean=x['x1'],sd=params['tau']),
      y2=rnorm(n=1,mean=x['x2'],sd=params['tau'])
      )
  }
@ 

Finally, we specify how to evaluate the likelihood of an observation given the underlying state.
Again, the arguments \code{x}, \code{y}, and \code{params} are vectors.
<<>>=
  bvnorm.dmeasure <- function (y, t, x, params, log, ...) {
    f <- sum(
             dnorm(
                   x=y[c("y1","y2")],
                   mean=x[c("x1","x2")],
                   sd=params["tau"],
                   log=TRUE
                   ),
             na.rm=TRUE
             )
    if (log) f else exp(f)
  }
@ 
With these four functions in hand, we construct the \code{pomp} object:
<<>>=
ou2 <- pomp( 
	    times=seq(1,100),
	    data=rbind(
	      y1=rep(0,100),
	      y2=rep(0,100)
	      ),
	    t0=0,
	    rprocess = ou2.rprocess,
	    dprocess = ou2.dprocess,
	    rmeasure = bvnorm.rmeasure,
	    dmeasure = bvnorm.dmeasure
	    )
@
In the above, \code{times} are the times at which the observations (given by \code{data}) were observed.
The scalar \code{t0} is the time at which the process model is initialized:
\code{t0} should not be any later than the first observation time \code{times[1]}.
In the present case, it was easy to specify all four of the basic functions.
That won't always be the case and it's not necessary to specify all of them to construct a \code{pomp} object.
If any statistical method using the \code{pomp} object wants access to a function that hasn't been provided, however, an error will be generated.

We'll now specify some ``true'' parameters and initial states in the form of a named numeric vector:
<<>>=
true.p <- c(
            alpha.1=0.9,alpha.2=0,alpha.3=0,alpha.4=0.99,
            sigma.1=1,sigma.2=0,sigma.3=2,
            tau=1,x1.0=50,x2.0=-50
            )
@ 
Note that the initial states are specified by parameters that have names ending in `.0'.
This is important, since this identifies them as initial-value parameters.
By default, the unobserved (state) process will be initialized with these values, and the names of the state variables will be obtained by dropping the `.0'.
In applications, one will frequently want more flexibility in parameterizing the initial state.
This is available: one can optionally specify an alternative initializer.
See \code{?pomp} for details.

<<echo=F,results=hide>>=
## this is a check to make sure the different implementations of 'ou2' are equivalent
x <- simulate(ou2,params=true.p,nsim=2,states=T,obs=T)
new.fp <- dprocess(ou2,x=x$states,params=cbind(true.p,true.p),times=time(ou2,t0=T),log=T) 
new.fm <- dmeasure(ou2,x=x$states,y=x$obs[,1,],params=cbind(true.p,true.p),times=time(ou2,t0=T),log=T) 

data(ou2)
old.fp <- dprocess(ou2,x=x$states,params=cbind(true.p,true.p),times=time(ou2,t0=T),log=T) 
old.fm <- dmeasure(ou2,x=x$states,y=x$obs[,1,],params=cbind(true.p,true.p),times=time(ou2,t0=T),log=T) 

if ((max(abs(old.fp-new.fp))>1e-12)||(max(abs(old.fm-new.fm))>1e-12)) stop("error in scratchy code")
@ 

The pomp object \code{ou2} we just constructed has no data.
If we simulate the model, we'll obtain another \code{pomp} object just like \code{ou2}, but with the \code{data} slot filled with simulated data:
<<>>=
 ou2 <- simulate(ou2,params=true.p,nsim=1000,seed=800733088)
 ou2 <- ou2[[1]]
@ 
Here, we actually ran 1000 simulations: the default behavior of \code{simulate} is to return a list of \code{pomp} objects.

\subsection{Methods of the \code{pomp} class.}

There are a number of \emph{methods} that perform operations on \code{pomp} objects.
One can read the documentation on all of these by doing \verb+class?pomp+ and \verb+methods?pomp+.
For example, one can coerce a \code{pomp} object to a data frame:
<<eval=F>>=
as(ou2,'data.frame')
@ 
and if we \code{print} a \code{pomp} object, the resulting data frame is what is shown.
One can access the data and the observation times using
<<eval=F>>=
data.array(ou2)
time(ou2)  
time(ou2,t0=TRUE)  
@ 
One can read and change parameters associated with the \code{pomp} object using
<<eval=F>>=
coef(ou2)
coef(ou2,c("sigma.1","sigma.2")) <- c(1,0)
@ 
One can also plot a \code{pomp} object (Fig.~\ref{fig:ou2}).

\begin{figure}
  \begin{center}
<<fig=T,echo=F>>=    
plot(ou2)
@ 
  \end{center}
  \caption{
    One can plot a \code{pomp} object.
    This shows the result of \code{plot(ou2)}.
  }
  \label{fig:ou2}
\end{figure}

\subsection{Building the \code{pomp} object using plugins.}

A fair amount of the difficulty in putting together the process model components above had to do with mundane bookkeeping:
constructing arrays of the appropriate dimensions, filling them in the right order, making sure that the names lined up.
The package provides some \emph{plug-in} facilities that allow the user to bypass these tedious and error-prone aspects.
At the moment, there are plug-ins to facilitate implementation of discrete-time dynamical systems and continuous-time systems via an Euler-type algorithm.
Let's see how we can implement the Ornstein-Uhlenbeck example using the discrete-time plugin.

<<>>=
ou2 <- pomp( 
	    times=seq(1,100),
	    data=rbind(
	      y1=rep(0,100),
	      y2=rep(0,100)
	      ),
	    t0=0,
            rprocess = onestep.simulate,
            dprocess = onestep.density,
            step.fun = function(x, t, params, delta.t, ...) {
              eps <- rnorm(n=2)
              with(
                   as.list(c(x,params)),
                   c(
                     x1=alpha.1*x1+alpha.3*x2+sigma.1*eps[1],
                     x2=alpha.2*x1+alpha.4*x2+sigma.2*eps[1]+sigma.3*eps[2]
                     )
                   )
            },
            dens.fun = function (x1, t1, x2, t2, params, ...) {
              eps.1 <- x2['x1']-params['alpha.1']*x1['x1']-
                         params['alpha.3']*x1['x2']
              eps.2 <- x2['x2']-params['alpha.2']*x1['x1']-
                         params['alpha.4']*x1['x2']-
                         params['sigma.2']/params['sigma.1']*eps.1
              sum(
                  dnorm(
                        c(eps.1,eps.2),
                        mean=0,
                        sd=params[c('sigma.1','sigma.3')],
                        log=T
                        ),
                  na.rm=T
                  )
            },
	    rmeasure = bvnorm.rmeasure,
	    dmeasure = bvnorm.dmeasure
	    )
@ 
The \code{rprocess} portion of the model is provided by the \code{onestep.simulate} plug-in.
One specifies \verb+rprocess=onestep.simulate+ and provides an additional argument \code{step.fun} that simulates one step of the process model given one state and one set of parameters.
The \code{dprocess} portion is specified using the \code{onestep.density} plug-in.
In this bit, one specifies \verb+dprocess=onestep.density+ and provides an additional argument \code{dens.fun} that computes the log pdf of a transition from \verb+(x1,t1)+ to \verb+(x2,t2)+ given the parameter vector \verb+params+.
Critically, in writing this function, one is allowed to assume that the transition from \verb+x1+ to \verb+x2+ is elementary, i.e., that no transition has occurred between times \verb+t1+ and \verb+t2+.
See the help page (\verb+?onestep.simulate+) for complete documentation.

<<echo=F,results=hide>>=
## this is a check to make sure the different implementations of 'ou2' are equivalent
x <- simulate(ou2,params=true.p,nsim=2,states=T,obs=T)
new.fp <- dprocess(ou2,x=x$states,params=cbind(true.p,true.p),times=time(ou2,t0=T),log=T) 
new.fm <- dmeasure(ou2,x=x$states,y=x$obs[,1,],params=cbind(true.p,true.p),times=time(ou2,t0=T),log=T) 

data(ou2)
old.fp <- dprocess(ou2,x=x$states,params=cbind(true.p,true.p),times=time(ou2,t0=T),log=T) 
old.fm <- dmeasure(ou2,x=x$states,y=x$obs[,1,],params=cbind(true.p,true.p),times=time(ou2,t0=T),log=T) 

if ((max(abs(old.fp-new.fp))>1e-12)||(max(abs(old.fm-new.fm))>1e-12)) stop("error in plugin code")
@ 

\section{Particle filtering.}

<<echo=F>>=
set.seed(74094853)
@ 

We can run a particle filter as follows:
<<>>=
data(ou2)
fit1 <- pfilter(ou2,params=true.p,Np=1000,filter.mean=T,pred.mean=T,pred.var=T)
@ 
The first (\code{data}) statement reads in the prepackaged version of \code{ou2}.
Since this \code{ou2} already contains the parameters \code{p}, it wasn't necessary to specify them;
we could have done
<<eval=F>>=
fit1 <- pfilter(ou2,Np=1000)
@ 
with much the same result, for example.

We can compare the results against those of the Kalman filter, which is exact in the case of a linear, Gaussian model such as the one implemented in \code{ou2}.
First, we need to implement the Kalman filter.
<<>>=
kalman.filter <- function (y, x0, a, b, sigma, tau) {
  n <- nrow(y)
  ntimes <- ncol(y)
  sigma.sq <- sigma%*%t(sigma)
  tau.sq <- tau%*%t(tau)
  inv.tau.sq <- solve(tau.sq)
  cond.dev <- numeric(ntimes)
  filter.mean <- matrix(0,n,ntimes)
  pred.mean <- matrix(0,n,ntimes)
  pred.var <- array(0,dim=c(n,n,ntimes))
  dev <- 0
  m <- x0
  v <- diag(0,n)
  for (k in seq(length=ntimes)) {
    pred.mean[,k] <- M <- a%*%m
    pred.var[,,k] <- V <- a%*%v%*%t(a)+sigma.sq
    q <- b%*%V%*%t(b)+tau.sq
    r <- y[,k]-b%*%M
    cond.dev[k] <- n*log(2*pi)+log(det(q))+t(r)%*%solve(q,r)
    dev <- dev+cond.dev[k]
    q <- t(b)%*%inv.tau.sq%*%b+solve(V)
    v <- solve(q)
    filter.mean[,k] <- m <- v%*%(t(b)%*%inv.tau.sq%*%y[,k]+solve(V,M))
  }
  list(
       pred.mean=pred.mean,
       pred.var=pred.var,
       filter.mean=filter.mean,
       cond.loglik=-0.5*cond.dev,
       loglik=-0.5*dev
       )
}
@ 
Now we can run it on the example data we generated above.
<<>>=
y <- data.array(ou2)
a <- matrix(true.p[c('alpha.1','alpha.2','alpha.3','alpha.4')],2,2)
b <- diag(1,2)   ## b is the identity matrix
sigma <- matrix(c(true.p['sigma.1'],true.p['sigma.2'],0,true.p['sigma.3']),2,2)
tau <- diag(true.p['tau'],2,2)
x0 <- init.state(ou2)
fit2 <- kalman.filter(y,x0,a,b,sigma,tau)
@ 
In this case, the Kalman filter gives us a log likelihood of \code{fit2\$loglik=\Sexpr{round(fit2$loglik,2)}}, while the particle filter gives us \code{fit1\$loglik=\Sexpr{round(fit1$loglik,2)}}.

\section{Iterated filtering: the MIF algorithm}

The MIF algorithm works by modifying the model slightly.
It replaces the model we are interested in fitting --- which has time-invariant parameters --- with a model that is just the same except that its parameters take a random walk in time.
As the intensity of this random walk approaches zero, the modified model approaches the fixed-parameter model.
MIF works by iterating a particle filter on this model.
The extra variability in the parameters combats the particle depletion that typically plagues simple particle filters.

At the beginning of each iteration, MIF must create an initial distribution of particles in the state-parameter space.
For this purpose, MIF uses a function, \code{particles}, which can be optionally specified by the user.
By default, MIF uses a multivariate normal particle distribution.
The \code{particles} function takes an argument, \code{sd}, that scales the width of the distribution of particles in each of the directions of the state-parameter space.
In particular, this distribution must be such that, when \code{sd=0}, all the particles are identical.
In this vignette, we'll use the default (multivariate normal) particle distribution.

Let's jump right in and run MIF to maximize the likelihood over two of the parameters and both initial conditions.
We'll use 1000 particles, an exponential cooling factor of 0.95, and a fixed-lag smoother with lag 10 for the initial conditions.
Just to make it interesting, we'll start far from the true parameter values.
<<>>=
start.p <- true.p
start.p[c('x1.0','x2.0','alpha.1','alpha.4')] <- c(45,-60,0.8,0.9)
fit <- mif(ou2,Nmif=1,start=start.p,
           pars=c('alpha.1','alpha.4'),ivps=c('x1.0','x2.0'),
           rw.sd=c(
             x1.0=5,x2.0=5,
             alpha.1=0.1,alpha.4=0.1
             ),
           Np=1000,
           var.factor=1,
           ic.lag=10,
           cooling.factor=0.95,
           max.fail=100
           )
fit <- continue(fit,Nmif=79,max.fail=100)
fitted.pars <- c("alpha.1","alpha.4","x1.0","x2.0")
cbind(
      start=start.p[fitted.pars],
      mle=signif(coef(fit,fitted.pars),3),
      truth=true.p[fitted.pars]
      )
@

One can plot various diagnostics for the fitted \code{mif} object using
<<eval=F>>=
plot(fit)
@ 
Here, we'll just plot the convergence records for the log likelihood and the two $\alpha$ parameters (Fig.~\ref{fig:convplot}).
In applications, a good strategy is to start several MIFs from different starting points.
A good diagnostic for convergence is obtained by plotting the \emph{convergence records} (see the documentation for \code{conv.rec}) and verifying that all the MIF iterations converge to the same parameters.
One plots these---and other---diagnostics using \code{compare.mif} applied to a list of \code{mif} objects.

\begin{figure}
<<fig=T,echo=F>>=
x <- conv.rec(fit,c("loglik","alpha.1","alpha.4"))
op <- par(fig=c(0,1,0.66,0.99),mar=c(0,4,4,0))
plot(x[,"loglik"],type='l',bty='l',xlab='',ylab=expression(log(L)),xaxt='n')
par(fig=c(0,1,0.33,0.66),mar=c(2,4,2,0),new=T)
plot(x[,"alpha.1"],type='l',bty='l',xlab='',ylab=expression(alpha[1]),xaxt='n')
par(fig=c(0,1,0.0,0.33),mar=c(4,4,0,0),new=T)
plot(x[,"alpha.4"],type='l',bty='l',xlab="MIF iteration",ylab=expression(alpha[4]))
par(op)
@ 
\caption{Convergence plots can be used to help diagnose convergence of the MIF algorithm.}
\label{fig:convplot}
\end{figure}

The log likelihood of the random-parameter model at the end of the mif iterations---which should be a rough approximation of that of the fixed-parameter model---is \code{logLik(fit)=\Sexpr{round(logLik(fit),1)}}.
To get the log likelihood of the fixed-parameter model (up to Monte Carlo error) we can use \code{pfilter}:
<<>>=
round(pfilter(fit)$loglik,1)
@ 
Like \code{pomp} objects, one can simulate from a fitted \code{mif} object (Fig.~\ref{fig:mifsim}).
In this case, the \code{pomp} is simulated at the MLE.

\begin{figure}
<<fig=T,echo=F>>=
plot(simulate(fit))
@ 
\caption{\code{mif} objects can be simulated.}  
\label{fig:mifsim}
\end{figure}

\section{Nonlinear forecasting}

<<echo=F,results=hide>>=
  options(glop)
@ 

\end{document}
